#!/usr/bin/python3
#-*- coding:utf-8 -*-

import os
import sys
import copy
import xlrd
import json
import tqdm
import shutil
import pickle
import random
import binascii
import operator
import numpy as np
import pandas as pd
import scapy.all as scapy
from functools import reduce
from flowcontainer.extractor import extract

random.seed(40)
word_dir = "/3241903007/workstation/AnomalyDetection/ET-BERT/corpora/"
word_name = "encrypted_traffic_burst.txt"

def convert_pcapng_2_pcap(pcapng_path, pcapng_file, output_path):
    
    pcap_file = output_path + pcapng_file.replace('pcapng','pcap')
    cmd = "editcap -F pcap %s %s"
    command = cmd%(pcapng_path+pcapng_file, pcap_file)
    os.system(command)
    return 0

def split_cap(pcap_path, pcap_file, pcap_name, pcap_label='', dataset_level = 'flow'):
    
    if not os.path.exists(pcap_path + "/splitcap"):
        os.mkdir(pcap_path + "/splitcap")
    if pcap_label != '':
        if not os.path.exists(pcap_path + "/splitcap/" + pcap_label):
            os.mkdir(pcap_path + "/splitcap/" + pcap_label)
        if not os.path.exists(pcap_path + "/splitcap/" + pcap_label + "/" + pcap_name):
            os.mkdir(pcap_path + "/splitcap/" + pcap_label + "/" + pcap_name)
   
        output_path = pcap_path + "/splitcap/" + pcap_label + "/" + pcap_name
    else:
        if not os.path.exists(pcap_path + "/splitcap/" + pcap_name):
            os.mkdir(pcap_path + "/splitcap/" + pcap_name)
        output_path = pcap_path + "/splitcap/" + pcap_name
    if dataset_level == 'flow':
        cmd = "mono /3241903007/workstation/AnomalyDetection/ET-BERT/tools/SplitCap.exe -r %s -s session -o " + output_path
    elif dataset_level == 'packet':
        cmd = "mono /3241903007/workstation/AnomalyDetection/ET-BERT/tools/SplitCap.exe -r %s -s packets 1 -o " + output_path
    command = cmd%pcap_file
    os.system(command)
    return output_path

def cut(obj, sec):
    result = [obj[i:i+sec] for i in range(0,len(obj),sec)]
    try:
        remanent_count = len(result[0])%4
    except Exception as e:
        remanent_count = 0
        print("cut datagram error!")
    if remanent_count == 0:
        pass
    else:
        result = [obj[i:i+sec+remanent_count] for i in range(0,len(obj),sec+remanent_count)]
    return result

def bigram_generation(packet_datagram, packet_len = 64, flag=True):
    result = ''
    generated_datagram = cut(packet_datagram,1)
    token_count = 0
    for sub_string_index in range(len(generated_datagram)):
        if sub_string_index != (len(generated_datagram) - 1):
            token_count += 1
            if token_count > packet_len:
                break
            else:
                merge_word_bigram = generated_datagram[sub_string_index] + generated_datagram[sub_string_index + 1]
        else:
            break
        result += merge_word_bigram
        result += ' '
    
    return result

def get_burst_feature(label_pcap, payload_len):
    feature_data = []
    
    packets = scapy.rdpcap(label_pcap)
    
    packet_direction = []
    feature_result = extract(label_pcap)
    for key in feature_result.keys():
        value = feature_result[key]
        packet_direction = [x // abs(x) for x in value.ip_lengths]

    if len(packet_direction) == len(packets):
        
        burst_data_string = ''
        
        burst_txt = ''
        
        for packet_index in range(len(packets)):
            packet_data = packets[packet_index].copy()
            data = (binascii.hexlify(bytes(packet_data)))
            
            packet_string = data.decode()[:2*payload_len]
            
            if packet_index == 0:
                burst_data_string += packet_string
            else:
                if packet_direction[packet_index] != packet_direction[packet_index - 1]:
                    
                    length = len(burst_data_string)
                    for string_txt in cut(burst_data_string, int(length / 2)):
                        burst_txt += bigram_generation(string_txt, packet_len=len(string_txt))
                        burst_txt += '\n'
                    burst_txt += '\n'
                    
                    burst_data_string = ''
                
                burst_data_string += packet_string
                if packet_index == len(packets) - 1:
                    
                    length = len(burst_data_string)
                    for string_txt in cut(burst_data_string, int(length / 2)):
                        burst_txt += bigram_generation(string_txt, packet_len=len(string_txt))
                        burst_txt += '\n'
                    burst_txt += '\n'
        
        with open(word_dir + word_name,'a') as f:
            f.write(burst_txt)
    return 0

def get_feature_packet(label_pcap,payload_len):
    feature_data = []

    packets = scapy.rdpcap(label_pcap)
    packet_data_string = ''  

    for packet in packets:
            packet_data = packet.copy()
            data = (binascii.hexlify(bytes(packet_data)))
            packet_string = data.decode()
            new_packet_string = packet_string[76:]
            packet_data_string += bigram_generation(new_packet_string, packet_len=payload_len, flag = True)
            break

    feature_data.append(packet_data_string)
    return feature_data

# def get_feature_flow(label_pcap, payload_len, payload_pac):
    
#     feature_data = []
#     packets = scapy.rdpcap(label_pcap)
#     packet_count = 0  
#     flow_data_string = '' 

#     feature_result = extract(label_pcap, filter='tcp', extension=['tls.record.content_type', 'tls.record.opaque_type', 'tls.handshake.type'])
#     if len(feature_result) == 0:
#         feature_result = extract(label_pcap, filter='udp')
#         if len(feature_result) == 0:
#             return -1
#         extract_keys = list(feature_result.keys())[0]
#         if len(feature_result[label_pcap, extract_keys[1], extract_keys[2]].ip_lengths) < 3:
#             print("preprocess flow %s but this flow has less than 3 packets." % label_pcap)
#             return -1
#     elif len(packets) < 3:
#         print("preprocess flow %s but this flow has less than 3 packets." % label_pcap)
#         return -1
#     try:
#         if len(feature_result[label_pcap, 'tcp', '0'].ip_lengths) < 3:
#             print("preprocess flow %s but this flow has less than 3 packets." % label_pcap)
#             return -1
#     except Exception as e:
#         print("*** this flow begings from 1 or other numbers than 0.")
#         for key in feature_result.keys():
#             if len(feature_result[key].ip_lengths) < 3:
#                 print("preprocess flow %s but this flow has less than 3 packets." % label_pcap)
#                 return -1

#     if feature_result.keys() == {}.keys():
#         return -1
    
#     if feature_result == {}:
#         return -1
#     feature_result_lens = len(feature_result.keys())
#     for key in feature_result.keys():
#         value = feature_result[key]

#     packet_index = 0
#     for packet in packets:
#         packet_count += 1
#         if packet_count == payload_pac:
#             packet_data = packet.copy()
#             data = (binascii.hexlify(bytes(packet_data)))
#             packet_string = data.decode()[76:]
#             flow_data_string += bigram_generation(packet_string, packet_len=payload_len, flag = True)
#             break
#         else:
#             packet_data = packet.copy()
#             data = (binascii.hexlify(bytes(packet_data)))
#             packet_string = data.decode()[76:]
#             flow_data_string += bigram_generation(packet_string, packet_len=payload_len, flag = True)
#     feature_data.append(flow_data_string)

#     return feature_data

# def generation(pcap_path, samples, features, splitcap = False, payload_length = 128, payload_packet = 5, dataset_save_path = "I:/ex_results/", dataset_level = "flow"):
#     if os.path.exists(dataset_save_path + "dataset.json"):
#         print("the pcap file of %s is finished generating."%pcap_path)
        
#         clean_dataset = 0
        
#         re_write = 0

#         if clean_dataset:
#             with open(dataset_save_path + "/dataset.json", "r") as f:
#                 new_dataset = json.load(f)
#             pop_keys = ['1','10','16','23','25','71']
#             print("delete domains.")
#             for p_k in pop_keys:
#                 print(new_dataset.pop(p_k))
            
#             change_keys = [str(x) for x in range(113, 119)]
#             relation_dict = {}
#             for c_k_index in range(len(change_keys)):
#                 relation_dict[change_keys[c_k_index]] = pop_keys[c_k_index]
#                 new_dataset[pop_keys[c_k_index]] = new_dataset.pop(change_keys[c_k_index])
#             with open(dataset_save_path + "/dataset.json", "w") as f:
#                 json.dump(new_dataset, fp=f, ensure_ascii=False, indent=4)
#         elif re_write:
#             with open(dataset_save_path + "/dataset.json", "r") as f:
#                 old_dataset = json.load(f)
#             os.renames(dataset_save_path + "/dataset.json", dataset_save_path + "/old_dataset.json")
#             with open(dataset_save_path + "/new-samples.txt", "r") as f:
#                 source_samples = f.read().split('\n')
#             new_dataset = {}
#             samples_count = 0
#             for i in range(len(source_samples)):
#                 current_class = source_samples[i].split('\t')
#                 if int(current_class[1]) > 9:
#                     new_dataset[str(samples_count)] = old_dataset[str(i)]
#                     samples_count += 1
#                     print(old_dataset[str(i)]['samples'])
#             with open(dataset_save_path + "/dataset.json", "w") as f:
#                 json.dump(new_dataset, fp=f, ensure_ascii=False, indent=4)
#         X, Y = obtain_data(pcap_path, samples, features, dataset_save_path)
#         return X,Y

#     dataset = {}
    
#     label_name_list = []

#     session_pcap_path  = {}

#     for parent, dirs, files in os.walk(pcap_path):
#         if label_name_list == []:
#             label_name_list.extend(dirs)
#         for dir in label_name_list:
#             for p,dd,ff in os.walk(parent + "/" + dir):
                
#                 if splitcap:
#                     for file in ff:
#                         session_path = (split_cap(pcap_path, p + "/" + file, file.split(".")[-2], dir, dataset_level = dataset_level))
#                     session_pcap_path[dir] = pcap_path + "/splitcap/" + dir
#                 else:
#                     session_pcap_path[dir] = pcap_path + dir
#         break
#     breakpoint()
#     print("all labels: %s"%(label_name_list))
#     label_id = {}
#     for index in range(len(label_name_list)):
#         label_id[label_name_list[index]] = index

#     r_file_record = []
#     print("\nBegin to generate features.")

#     label_count = 0
#     for key in tqdm.tqdm(session_pcap_path.keys()):

#         if dataset_level == "flow":
#             if splitcap:
#                 for p, d, f in os.walk(session_pcap_path[key]):
#                     for file in f:
#                         file_size = float(size_format(os.path.getsize(p + "/" + file)))
#                         # 2KB
#                         if file_size < 2:
#                             os.remove(p + "/" + file)
#                             print("remove sample: %s for its size is less than 2 KB." % (p + "/" + file))

#             if label_id[key] not in dataset:
#                 dataset[label_id[key]] = {
#                     "samples": 0,
#                     "payload": {},
#                     "length": {},
#                     "time": {},
#                     "direction": {},
#                     "message_type": {}
#                 }
#         elif dataset_level == "packet":
#             if splitcap:# not splitcap
#                 for p, d, f in os.walk(session_pcap_path[key]):
#                     for file in f:
#                         current_file = p + "/" + file
#                         if not os.path.getsize(current_file):
#                             os.remove(current_file)
#                             print("current pcap %s is 0KB and delete"%current_file)
#                         else:
#                             current_packet = scapy.rdpcap(p + "/" + file)
#                             file_size = float(size_format(os.path.getsize(p + "/" + file)))
#                             try:
#                                 if 'TCP' in str(current_packet.res):
#                                     # 0.12KB
#                                     if file_size < 0.14:
#                                         os.remove(p + "/" + file)
#                                         print("remove TCP sample: %s for its size is less than 0.14KB." % (
#                                                     p + "/" + file))
#                                 elif 'UDP' in str(current_packet.res):
#                                     if file_size < 0.1:
#                                         os.remove(p + "/" + file)
#                                         print("remove UDP sample: %s for its size is less than 0.1KB." % (
#                                                     p + "/" + file))
#                             except Exception as e:
#                                 print("error in data_generation 611: scapy read pcap and analyse error")
#                                 os.remove(p + "/" + file)
#                                 print("remove packet sample: %s for reading error." % (p + "/" + file))
#             if label_id[key] not in dataset:
#                 dataset[label_id[key]] = {
#                     "samples": 0,
#                     "payload": {}
#                 }
#         if splitcap:
#             continue

#         target_all_files = [x[0] + "/" + y for x in [(p, f) for p, d, f in os.walk(session_pcap_path[key])] for y in x[1]]
#         breakpoint()
#         need = samples[label_count]
#         available = len(target_all_files)

#         if need > available:
#             print(f"[Warning] label {label_count}: need {need}, but only {available} available. Using all available.")
#             need = available
#         elif need < 0:
#             raise ValueError(f"Invalid sample size {need} for label {label_count}")

#         r_files = random.sample(target_all_files, need)
#         # r_files = random.sample(target_all_files, samples[label_count])
#         label_count += 1
#         for r_f in r_files:
#             if dataset_level == "flow":
#                 # feature_data = get_feature_flow(r_f, payload_len=payload_length, payload_pac=payload_packet)
#                 payload, lengths, directions, iats = get_feature_flow(r_f, payload_len=payload_length, payload_pac=payload_packet)
#             elif dataset_level == "packet":
#                 feature_data = get_feature_packet(r_f, payload_len=payload_length)

#             if feature_data == -1:
#                 continue
#             r_file_record.append(r_f)
#             dataset[label_id[key]]["samples"] += 1
#             if len(dataset[label_id[key]]["payload"].keys()) > 0:
#                 dataset[label_id[key]]["payload"][str(dataset[label_id[key]]["samples"])] = \
#                     feature_data[0]
#                 if dataset_level == "flow":
#                     pass
#             else:
#                 dataset[label_id[key]]["payload"]["1"] = feature_data[0]
#                 if dataset_level == "flow":
#                     pass

#     all_data_number = 0
#     for index in range(len(label_name_list)):
#         print("%s\t%s\t%d"%(label_id[label_name_list[index]], label_name_list[index], dataset[label_id[label_name_list[index]]]["samples"]))
#         all_data_number += dataset[label_id[label_name_list[index]]]["samples"]
#     print("all\t%d"%(all_data_number))

#     with open(dataset_save_path + "/picked_file_record","w") as p_f:
#         for i in r_file_record:
#             p_f.write(i)
#             p_f.write("\n")
#     with open(dataset_save_path + "/dataset.json", "w") as f:
#         json.dump(dataset,fp=f,ensure_ascii=False,indent=4)

#     X,Y = obtain_data(pcap_path, samples, features, dataset_save_path, json_data = dataset)
#     return X,Y
def get_feature_flow(label_pcap, payload_len, payload_pac):
    import scapy.all as scapy
    import binascii
    import numpy as np
    from flowcontainer.extractor import extract

    feature_data = []
    packets = scapy.rdpcap(label_pcap)
    packet_count = 0
    flow_data_string = ''

    feature_result = extract(label_pcap, filter='tcp', extension=['tls.record.content_type', 'tls.record.opaque_type', 'tls.handshake.type'])
    if len(feature_result) == 0:
        feature_result = extract(label_pcap, filter='udp')
        if len(feature_result) == 0:
            return -1
        extract_keys = list(feature_result.keys())[0]
        if len(feature_result[label_pcap, extract_keys[1], extract_keys[2]].ip_lengths) < 3:
            print("preprocess flow %s but this flow has less than 3 packets." % label_pcap)
            return -1
    elif len(packets) < 3:
        print("preprocess flow %s but this flow has less than 3 packets." % label_pcap)
        return -1
    try:
        if len(feature_result[label_pcap, 'tcp', '0'].ip_lengths) < 3:
            print("preprocess flow %s but this flow has less than 3 packets." % label_pcap)
            return -1
    except Exception as e:
        print("*** this flow begings from 1 or other numbers than 0.")
        for key in feature_result.keys():
            if len(feature_result[key].ip_lengths) < 3:
                print("preprocess flow %s but this flow has less than 3 packets." % label_pcap)
                return -1

    if feature_result.keys() == {}.keys() or feature_result == {}:
        return -1

    # 选第一个 flow
    flow_key = list(feature_result.keys())[0]
    flow_obj = feature_result[flow_key]

    lengths = np.array(flow_obj.ip_lengths).tolist()
    directions = [int(np.sign(x)) for x in flow_obj.ip_lengths]
    timestamps = np.array(flow_obj.timestamps)
    # iats = np.diff(timestamps).tolist() if len(timestamps) > 1 else [0]
    # 只保留两位小数
    iats = [round(x, 2) for x in np.diff(timestamps)] if len(timestamps) > 1 else [0.0]

    # payload 部分保持不变
    for packet in packets:
        packet_count += 1
        packet_data = packet.copy()
        data = (binascii.hexlify(bytes(packet_data)))
        packet_string = data.decode()[76:]
        flow_data_string += bigram_generation(packet_string, packet_len=payload_len, flag=True)
        if packet_count == payload_pac:
            break

    # ✅ 关键修改：返回四个特征
    return flow_data_string, lengths, directions, iats

# def generation(
#     pcap_path,
#     samples,
#     features,
#     splitcap=False,
#     payload_length=128,
#     payload_packet=5,
#     dataset_save_path="I:/ex_results/",
#     dataset_level="flow"
# ):
#     if os.path.exists(dataset_save_path + "dataset.json"):
#         print("the pcap file of %s is finished generating." % pcap_path)
#         clean_dataset = 0
#         re_write = 0

#         if clean_dataset:
#             with open(dataset_save_path + "/dataset.json", "r") as f:
#                 new_dataset = json.load(f)
#             pop_keys = ['1','10','16','23','25','71']
#             print("delete domains.")
#             for p_k in pop_keys:
#                 print(new_dataset.pop(p_k))
            
#             change_keys = [str(x) for x in range(113, 119)]
#             relation_dict = {}
#             for c_k_index in range(len(change_keys)):
#                 relation_dict[change_keys[c_k_index]] = pop_keys[c_k_index]
#                 new_dataset[pop_keys[c_k_index]] = new_dataset.pop(change_keys[c_k_index])
#             with open(dataset_save_path + "/dataset.json", "w") as f:
#                 json.dump(new_dataset, fp=f, ensure_ascii=False, indent=4)
#         elif re_write:
#             with open(dataset_save_path + "/dataset.json", "r") as f:
#                 old_dataset = json.load(f)
#             os.renames(dataset_save_path + "/dataset.json", dataset_save_path + "/old_dataset.json")
#             with open(dataset_save_path + "/new-samples.txt", "r") as f:
#                 source_samples = f.read().split('\n')
#             new_dataset = {}
#             samples_count = 0
#             for i in range(len(source_samples)):
#                 current_class = source_samples[i].split('\t')
#                 if int(current_class[1]) > 9:
#                     new_dataset[str(samples_count)] = old_dataset[str(i)]
#                     samples_count += 1
#                     print(old_dataset[str(i)]['samples'])
#             with open(dataset_save_path + "/dataset.json", "w") as f:
#                 json.dump(new_dataset, fp=f, ensure_ascii=False, indent=4)
#         X, Y = obtain_data(pcap_path, samples, features, dataset_save_path)
#         return X, Y

#     dataset = {}

#     # -------------------
#     # 获取类别文件夹作为标签
#     # -------------------
#     label_name_list = [d for d in os.listdir(pcap_path) if os.path.isdir(os.path.join(pcap_path, d))]
#     label_name_list.sort()  # 保证 label ID 顺序固定

#     # 构建类别对应的路径
#     session_pcap_path = {label: os.path.join(pcap_path, label) for label in label_name_list}

#     # 构建 label ID
#     label_id = {label: i for i, label in enumerate(label_name_list)}

#     r_file_record = []
#     breakpoint()

#     print("\nBegin to generate features.")

#     label_count = 0
#     for key in tqdm.tqdm(session_pcap_path.keys()):

#         # -------------------
#         # 初始化每个类别的存储结构
#         # -------------------
#         if dataset_level == "flow":
#             if label_id[key] not in dataset:
#                 dataset[label_id[key]] = {
#                     "samples": 0,
#                     "payload": {},
#                     "length": {},
#                     "time": {},
#                     "direction": {},
#                     "message_type": {}
#                 }

#         # -------------------
#         # 获取对应类别的所有流文件
#         # -------------------
#         target_all_files = [
#             x[0] + "/" + y
#             for x in [(p, f) for p, d, f in os.walk(session_pcap_path[key])]
#             for y in x[1]
#         ]

#         need = samples[label_count]
#         available = len(target_all_files)

#         if need > available:
#             print(f"[Warning] label {label_count}: need {need}, but only {available} available. Using all available.")
#             need = available
#         elif need < 0:
#             raise ValueError(f"Invalid sample size {need} for label {label_count}")

#         r_files = random.sample(target_all_files, need)
#         label_count += 1

#         # -------------------
#         # 处理每个流
#         # -------------------
#         for r_f in r_files:
#             if dataset_level == "flow":
#                 res = get_feature_flow(r_f, payload_len=payload_length, payload_pac=payload_packet)
#                 print(r_f)
#                 if res == -1:
#                     continue
#                 payload, lengths, directions, iats = res
#             else:
#                 continue

#             r_file_record.append(r_f)
#             dataset[label_id[key]]["samples"] += 1
#             idx = str(dataset[label_id[key]]["samples"])

#             dataset[label_id[key]]["payload"][idx] = payload
#             dataset[label_id[key]]["length"][idx] = lengths
#             dataset[label_id[key]]["direction"][idx] = directions
#             dataset[label_id[key]]["time"][idx] = iats

#     # -------------------
#     # 输出统计结果
#     # -------------------
#     all_data_number = 0
#     for index in range(len(label_name_list)):
#         print(
#             "%s\t%s\t%d"
#             % (
#                 label_id[label_name_list[index]],
#                 label_name_list[index],
#                 dataset[label_id[label_name_list[index]]]["samples"],
#             )
#         )
#         all_data_number += dataset[label_id[label_name_list[index]]]["samples"]
#     print("all\t%d" % (all_data_number))

#     # -------------------
#     # 保存结果
#     # -------------------
#     with open(dataset_save_path + "/picked_file_record", "w") as p_f:
#         for i in r_file_record:
#             p_f.write(i + "\n")

#     with open(dataset_save_path + "/dataset.json", "w") as f:
#         json.dump(dataset, fp=f, ensure_ascii=False, indent=4)

#     X, Y = obtain_data(pcap_path, samples, features, dataset_save_path, json_data=dataset)
#     return X, Y

def generation(
    pcap_path,
    samples,
    features,
    splitcap=False,
    payload_length=128,
    payload_packet=5,
    dataset_save_path="I:/ex_results/",
    dataset_level="flow"
):
    import json, os, tqdm, random

    progress_file = os.path.join(dataset_save_path, "progress.txt")

    # ===== 读取已完成记录 =====
    done_files = set()
    if os.path.exists(progress_file):
        with open(progress_file, "r") as f:
            done_files = set(line.strip().replace("完成 ", "") for line in f if line.strip())

    # ===== 如果已存在数据集，则直接读取 =====
    if os.path.exists(os.path.join(dataset_save_path, "dataset.json")):
        print(f"[INFO] {pcap_path} 已经生成 dataset.json，跳过。")
        X, Y = obtain_data(pcap_path, samples, features, dataset_save_path)
        return X, Y

    dataset = {}

    # ------------------- 获取类别文件夹 -------------------
    label_name_list = [d for d in os.listdir(pcap_path) if os.path.isdir(os.path.join(pcap_path, d))]
    label_name_list.sort()
    session_pcap_path = {label: os.path.join(pcap_path, label) for label in label_name_list}
    label_id = {label: i for i, label in enumerate(label_name_list)}

    r_file_record = []
    print("\n[INFO] Begin to generate features.")

    label_count = 0
    for key in tqdm.tqdm(session_pcap_path.keys()):

        if dataset_level == "flow":
            if label_id[key] not in dataset:
                dataset[label_id[key]] = {
                    "samples": 0,
                    "payload": {},
                    "length": {},
                    "time": {},
                    "direction": {},
                    "message_type": {}
                }

        target_all_files = [
            x[0] + "/" + y
            for x in [(p, f) for p, d, f in os.walk(session_pcap_path[key])]
            for y in x[1]
        ]

        need = samples[label_count]
        available = len(target_all_files)

        if need > available:
            print(f"[Warning] label {label_count}: need {need}, but only {available} available. Using all available.")
            need = available
        elif need < 0:
            raise ValueError(f"Invalid sample size {need} for label {label_count}")

        r_files = random.sample(target_all_files, need)
        label_count += 1

        for r_f in r_files:
            # ===== 跳过已完成的文件 =====
            if r_f in done_files:
                print(f"[跳过已完成] {r_f}")
                continue

            if dataset_level == "flow":
                res = get_feature_flow(r_f, payload_len=payload_length, payload_pac=payload_packet)
                if res == -1:
                    continue
                payload, lengths, directions, iats = res
            else:
                continue

            r_file_record.append(r_f)
            dataset[label_id[key]]["samples"] += 1
            idx = str(dataset[label_id[key]]["samples"])
            dataset[label_id[key]]["payload"][idx] = payload
            dataset[label_id[key]]["length"][idx] = lengths
            dataset[label_id[key]]["direction"][idx] = directions
            dataset[label_id[key]]["time"][idx] = iats

            # ===== ✅ 写入进度文件 =====
            with open(progress_file, "a") as f:
                f.write(f"完成 {r_f}\n")

            # ===== ✅ 定期保存中间结果 =====
            if dataset[label_id[key]]["samples"] % 100 == 0:
                with open(os.path.join(dataset_save_path, "dataset_tmp.json"), "w") as f:
                    json.dump(dataset, fp=f, ensure_ascii=False, indent=4)

    # ------------------- 输出统计结果 -------------------
    all_data_number = 0
    for index in range(len(label_name_list)):
        print(
            "%s\t%s\t%d"
            % (
                label_id[label_name_list[index]],
                label_name_list[index],
                dataset[label_id[label_name_list[index]]]["samples"],
            )
        )
        all_data_number += dataset[label_id[label_name_list[index]]]["samples"]
    print("all\t%d" % (all_data_number))

    # ------------------- 保存结果 -------------------
    with open(os.path.join(dataset_save_path, "picked_file_record"), "w") as p_f:
        for i in r_file_record:
            p_f.write(i + "\n")

    with open(os.path.join(dataset_save_path, "dataset.json"), "w") as f:
        json.dump(dataset, fp=f, ensure_ascii=False, indent=4)

    X, Y = obtain_data(pcap_path, samples, features, dataset_save_path, json_data=dataset)
    return X, Y
    
def read_data_from_json(json_data, features, samples):
    X,Y = [], []
    ablation_flag = 0
    for feature_index in range(len(features)):
        x = []
        label_count = 0
        for label in json_data.keys():
            sample_num = json_data[label]["samples"]
            if X == []:
                if not ablation_flag:
                    y = [label] * sample_num
                    Y.append(y)
                else:
                    if sample_num > 1500:
                        y = [label] * 1500
                    else:
                        y = [label] * sample_num
                    Y.append(y)
            if samples[label_count] < sample_num:
                x_label = []
                for sample_index in random.sample(list(json_data[label][features[feature_index]].keys()),1500):
                    # x_label.append(json_data[label][features[feature_index]][sample_index])
                    x_label.append({
                        "payload": json_data[label]["payload"][sample_index],
                        "length": json_data[label]["length"][sample_index],
                        "direction": json_data[label]["direction"][sample_index],
                        "time": json_data[label]["time"][sample_index]
                    })
                x.append(x_label)
            else:
                x_label = []
                for sample_index in json_data[label][features[feature_index]].keys():
                    # x_label.append(json_data[label][features[feature_index]][sample_index])
                     x_label.append({
                        "payload": json_data[label]["payload"][sample_index],
                        "length": json_data[label]["length"][sample_index],
                        "direction": json_data[label]["direction"][sample_index],
                        "time": json_data[label]["time"][sample_index]
                    })
                x.append(x_label)
            label_count += 1
        X.append(x)
    return X,Y

def obtain_data(pcap_path, samples, features, dataset_save_path, json_data = None):
    
    if json_data:
        X,Y = read_data_from_json(json_data,features,samples)
    else:
        print("read dataset from json file.")
        with open(dataset_save_path + "/dataset.json","r") as f:
            dataset = json.load(f)
        X,Y = read_data_from_json(dataset,features,samples)

    for index in range(len(X)):
        if len(X[index]) != len(Y):
            print("data and labels are not properly associated.")
            print("x:%s\ty:%s"%(len(X[index]),len(Y)))
            return -1
    return X,Y

def combine_dataset_json():
    dataset_name = "I:/traffic_pcap/splitcap/dataset-"
    # dataset vocab
    dataset = {}
    # progress
    progress_num = 8
    for i in range(progress_num):
        dataset_file = dataset_name + str(i) + ".json"
        with open(dataset_file,"r") as f:
            json_data = json.load(f)
        for key in json_data.keys():
            if i > 1:
                new_key = int(key) + 9*1 + 6*(i-1)
            else:
                new_key = int(key) + 9*i
            print(new_key)
            if new_key not in dataset.keys():
                dataset[new_key] = json_data[key]
    with open("I:/traffic_pcap/splitcap/dataset.json","w") as f:
        json.dump(dataset, fp=f, ensure_ascii=False, indent=4)
    return 0

def pretrain_dataset_generation(pcap_path):
    output_split_path = "/3241903007/workstation/AnomalyDetection/ET-BERT/datasets/USTC-TFC2016/output_split/"
    pcap_output_path = "/3241903007/workstation/AnomalyDetection/ET-BERT/datasets/USTC-TFC2016/pcap_output/"
    
    if not os.listdir(pcap_output_path):
        print("Begin to convert pcapng to pcap.")
        for _parent,_dirs,files in os.walk(pcap_path):
            for file in files:
                if 'pcapng' in file:
                    #print(_parent + file)
                    convert_pcapng_2_pcap(_parent, file, pcap_output_path)
                else:
                    shutil.copy(_parent+"/"+file, pcap_output_path+file)
    
    if not os.path.exists(output_split_path + "splitcap"):
        print("Begin to split pcap as session flows.")
        
        for _p,_d,files in os.walk(pcap_output_path):
            for file in files:
                split_cap(output_split_path,_p+file,file)
    print("Begin to generate burst dataset.")
    # burst sample
    for _p,_d,files in os.walk(output_split_path + "splitcap"):
        for file in files:
            get_burst_feature(_p+"/"+file, payload_len=64)
    return 0

def size_format(size):
    # 'KB'
    file_size = '%.3f' % float(size/1000)
    return file_size

if __name__ == '__main__':
    # pretrain
    pcap_path = "/3241903007/workstation/AnomalyDetection/ET-BERT/datasets/USTC-TFC2016/pcap/"
    # tls 13 downstream
    #pcap_path, samples, features = "I:/dataset/labeled/", 500, ["payload","length","time","direction","message_type"]
    #X,Y = generation(pcap_path, samples, features, splitcap=False)
    # pretrain data
    pretrain_dataset_generation(pcap_path)
    #print("X:%s\tx:%s\tY:%s"%(len(X),len(X[0]),len(Y)))
    # combine dataset.json
    #combine_dataset_json()
